
## **Ãrvore de DecisÃ£o**


Ã‰ como separa os dados em conjunto homogeneo, os nÃ³s, tendo como maior ganho para que seja ideal. Separar em "Baldes" aquele conjunto mais homogeneo, porem como foi falado nao deve ficar muito personalizado para modelo de machine learning que ai tb nao vai funcionar adequadamente 

Claro, Andre! Vamos falar de forma **bem didÃ¡tica** sobre o algoritmo de **Ãrvore de DecisÃ£o**, que Ã© um dos mais simples e intuitivos em **machine learning**.

---

## ğŸŒ³ O que Ã© uma Ãrvore de DecisÃ£o?

Uma **Ãrvore de DecisÃ£o** Ã© um modelo que **simula decisÃµes humanas**, como se fosse um fluxograma. Ela serve para **classificar** ou **prever** valores com base em perguntas **simples** e **binÃ¡rias** (sim/nÃ£o, maior/menor, etc.).

---

### ğŸ” Como funciona?

Ela divide os dados em **ramificaÃ§Ãµes**, fazendo perguntas sobre os atributos dos dados, atÃ© chegar a uma **decisÃ£o final** (folha da Ã¡rvore).

---

## ğŸ¯ Exemplo simples: "Vamos sair para passear?"

Imagina que queremos decidir **se devemos sair para passear** ou nÃ£o, com base em 3 fatores:

- â˜ï¸ EstÃ¡ chovendo?
- ğŸ“ˆ Temperatura estÃ¡ acima de 20Â°C?
- ğŸ“… Ã‰ fim de semana?

---

### ğŸŒ³ Ãrvore de decisÃ£o (simples):

```text
                 EstÃ¡ chovendo?
                    /     \
                 Sim       NÃ£o
                /           \
           NÃ£o saÃ­mos     Temperatura > 20?
                            /         \
                         Sim         NÃ£o
                        /              \
                Ã‰ fim de semana?     NÃ£o saÃ­mos
                    /     \
                 Sim       NÃ£o
               SaÃ­mos     NÃ£o saÃ­mos
```

---

### âœ… Testando com um exemplo:

- EstÃ¡ chovendo? âŒ (nÃ£o)
- Temperatura > 20Â°C? âœ… (sim)
- Ã‰ fim de semana? âœ… (sim)

â¡ï¸ Resultado final: **SaÃ­mos!**

---

## ğŸ§  Como o algoritmo monta essa Ã¡rvore?

Ele escolhe **as melhores perguntas (atributos)** para dividir os dados usando critÃ©rios como:

- **Ganho de InformaÃ§Ã£o (Information Gain)**
- **Ãndice de Gini**

Esses critÃ©rios medem **quÃ£o bem uma pergunta separa os dados** em grupos homogÃªneos.

---

## ğŸ›  AplicaÃ§Ãµes na prÃ¡tica

- DiagnÃ³stico mÃ©dico ğŸ¥
- AnÃ¡lise de crÃ©dito ğŸ’³
- DetecÃ§Ã£o de fraudes ğŸ”
- PrevisÃ£o de vendas ğŸ“Š

---

## âš–ï¸ Vantagens

- FÃ¡cil de entender e visualizar.
- Funciona com dados categÃ³ricos e numÃ©ricos.
- Pouco prÃ©-processamento.

---

## âš ï¸ Desvantagens

- Pode se tornar muito complexa (overfitting).
- SensÃ­vel a pequenas mudanÃ§as nos dados.

---

Se quiser, posso te mostrar um exemplo em **Python com sklearn**, usando dados reais como o famoso dataset de **Iris** ou de **Titanic**. Quer ver um desses exemplos?


Show! Vamos usar o dataset do **Titanic** ğŸ›³ï¸, que Ã© clÃ¡ssico para aprendizado de modelos de classificaÃ§Ã£o.

---

## ğŸ¯ Objetivo:
Prever se uma pessoa **sobreviveu** ou **nÃ£o**, com base em algumas caracterÃ­sticas (idade, sexo, classe do navio, etc).

Vamos usar o modelo de **Ãrvore de DecisÃ£o** do `scikit-learn`.

---

## ğŸ“¦ Instale se precisar:
```bash
pip install pandas scikit-learn seaborn matplotlib
```

---

## ğŸ’» CÃ³digo em Python:

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Carregando o dataset do Titanic
df = sns.load_dataset("titanic")

# Selecionando colunas importantes e removendo linhas com valores ausentes
df = df[["survived", "pclass", "sex", "age"]].dropna()

# Convertendo variÃ¡vel categÃ³rica 'sex' para numÃ©rica
df["sex"] = df["sex"].map({"male": 0, "female": 1})

# Separando X (atributos) e y (alvo)
X = df[["pclass", "sex", "age"]]
y = df["survived"]

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Criando o modelo de Ãrvore de DecisÃ£o
modelo = DecisionTreeClassifier(max_depth=3, random_state=42)
modelo.fit(X_train, y_train)

# Fazendo previsÃµes
y_pred = modelo.predict(X_test)

# Avaliando o modelo
print(f"AcurÃ¡cia: {accuracy_score(y_test, y_pred):.2f}")

# Plotando a Ã¡rvore
plt.figure(figsize=(12, 6))
plot_tree(modelo, feature_names=X.columns, class_names=["NÃ£o Sobreviveu", "Sobreviveu"], filled=True)
plt.title("Ãrvore de DecisÃ£o - Titanic")
plt.show()
```

---

### ğŸ§  InterpretaÃ§Ã£o da Ãrvore

- O modelo decide, por exemplo:
  - Se o passageiro for mulher (`sex=1`) e tiver menos de certa idade, hÃ¡ maior chance de sobreviver.
  - Passageiros da **primeira classe** tinham maior chance de sobrevivÃªncia.
  - Homens da **terceira classe** tinham menor chance.

---
